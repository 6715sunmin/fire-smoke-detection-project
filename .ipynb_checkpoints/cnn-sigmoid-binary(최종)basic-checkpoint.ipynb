{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a78bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d69a75",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (515981625.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 22\u001b[1;36m\u001b[0m\n\u001b[1;33m    print('Using device: ', device)\u001b[0m\n\u001b[1;37m                                   \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def my_seed_everywhere(seed: int = 42):\n",
    "    random.seed(seed) # random\n",
    "    np.random.seed(seed) # np\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed) # os\n",
    "    tf.random.set_seed(seed) # tensorflow\n",
    "\n",
    "my_seed = 42\n",
    "my_seed_everywhere(my_seed)\n",
    "\n",
    "# Set up some global variables\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e40382",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH =  os.getcwd()\n",
    "\n",
    "data = pd.read_csv(PATH +  '\\\\smoke_detection_iot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed28503",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57233f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0', 'UTC'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb418a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Fire Alarm'].values\n",
    "x = data.drop('Fire Alarm', axis=1)\n",
    "\n",
    "print('x shape : ',x.shape)\n",
    "print('y shape : ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = data.groupby('Fire Alarm').mean().iloc[:, :]\n",
    "data_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.imshow(np.array(data_group.iloc[i]).reshape(13,1))\n",
    "    plt.title(f'Fire Alarm : {i}')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.xticks(fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e39e90",
   "metadata": {},
   "source": [
    "# SCALER AND SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ae152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=11)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea393f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robustScaler = RobustScaler()\n",
    "robustScaler.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = robustScaler.transform(x_train)\n",
    "x_val = robustScaler.transform(x_val)\n",
    "x_test = robustScaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_rob = np.array(x_train_rob).reshape(-1, 13, 1, 1)\n",
    "#x_val_rob = np.array(x_val_rob).reshape(-1, 13, 1, 1)\n",
    "#x_test_rob = np.array(x_test_rob).reshape(-1, 13, 1, 1)\n",
    "x_train = np.array(x_train)\n",
    "x_val = np.array(x_val)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print('x_train shape : ',x_train.shape)\n",
    "print('x_val shape : ',x_val.shape)\n",
    "print('x_test shape : ',x_test.shape)\n",
    "print('y_train shape : ',y_train.shape)\n",
    "print('y_val shape : ',y_val.shape)\n",
    "print('y_test shape : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389efe77",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, x, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert x.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.x, self.y = x, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.x.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.x[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "train_dset = Dataset(x_train[:, :, np.newaxis], y_train, batch_size=32, shuffle=True)\n",
    "val_dset = Dataset(x_val[:, :, np.newaxis], y_val, batch_size=32, shuffle=False)\n",
    "test_dset = Dataset(x_test[:, :, np.newaxis], y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during training\n",
    "    \"\"\"    \n",
    "    with tf.device(device):\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        \n",
    "        model = model_init_fn()  # 모델 초기화 함수 호출하여 모델 인스턴스 생성\n",
    "        optimizer = optimizer_init_fn()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
    "        \n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            \n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(tf.expand_dims(y_np, axis=-1), scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(tf.expand_dims(test_y, axis=-1), prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print(template.format(t, epoch+1,\n",
    "                                              train_loss.result(),\n",
    "                                              train_accuracy.result()*100,\n",
    "                                              val_loss.result(),\n",
    "                                              val_accuracy.result()*100))\n",
    "                    t += 1\n",
    "                \n",
    "            train_losses.append(train_loss.result())\n",
    "            train_accuracies.append(train_accuracy.result())\n",
    "            val_losses.append(val_loss.result())\n",
    "            val_accuracies.append(val_accuracy.result())\n",
    "            \n",
    "        # Plotting the results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e12f34",
   "metadata": {},
   "source": [
    "# sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c261a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        \n",
    "        channel_1 = 32\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv1D(channel_1, kernel_size=5, strides=1, padding='valid',\n",
    "                                            activation='relu', kernel_initializer=initializer)\n",
    "        self.maxpool1 = tf.keras.layers.MaxPooling1D(pool_size=1)\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='sigmoid', kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=initializer)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        #x = tf.expand_dims(input_tensor, axis=-1)  # Add a new dimension for the channels\n",
    "        #print(input_tensor)\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #probs = tf.nn.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print_every = 100\n",
    "num_epochs = 4\n",
    "\n",
    "model = CustomConvNet()\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomConvNet()\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 0.0025\n",
    "    return tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc045cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dset):\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    for test_x, test_y in test_dset:\n",
    "        prediction = model(test_x, training=False)\n",
    "        t_loss = loss_fn(tf.expand_dims(test_y, axis=-1), prediction)\n",
    "        \n",
    "        test_loss.update_state(t_loss)\n",
    "        test_accuracy.update_state(test_y, prediction)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss.result()}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy.result() * 100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf67da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have defined the `test_dset` dataset\n",
    "test_model(model, test_dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d736eac",
   "metadata": {},
   "source": [
    "batch16 epoch8 : 40.523712158203125, 0.8191782236099243\n",
    "\n",
    "batch16 epoch4 : 40.523712158203125, 0.8191782236099243"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae79001",
   "metadata": {},
   "source": [
    "batch32 epoch8 : \n",
    "\n",
    "batch32 epoch4 : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c3568",
   "metadata": {},
   "source": [
    "batch64 epoch8 : \n",
    "    \n",
    "batch64 epoch4 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43815b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684bf93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e57ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a643117b",
   "metadata": {},
   "source": [
    "batch64 epoch4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbb917",
   "metadata": {},
   "source": [
    "lr 0.0025 : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06d7ec",
   "metadata": {},
   "source": [
    "lr 0.0005 : 40.523712158203125, 0.8192527294158936"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899d95b",
   "metadata": {},
   "source": [
    "lr 0.0001 : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129f70e",
   "metadata": {},
   "source": [
    "lr 0.005 : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2a4de",
   "metadata": {},
   "source": [
    "lr 0.0001 : "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
